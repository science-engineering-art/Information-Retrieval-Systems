% This is LLNCS.DOC the documentation file of
% the LaTeX2e class from Springer-Verlag
% for Lecture Notes in Computer Science, version 2.4
\documentclass{llncs}
\usepackage{Proyecto_Final_Sherlock}
\usepackage{color}


\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{amsfonts}
\usepackage{todonotes}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}

%\usepackage{amsthm}
\usepackage{amsmath}
\newtheorem{defn}[theorem]{Definici\'on}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage[spanish]{babel}

% Keywords command
\providecommand{\keywords}[1]
{
	\small	
	\textbf{\textit{Palabras clave --- }} #1
}

\begin{document}
	\markboth{Sistemas de Recuperaci\'on de Informaci\'on}{	Motor de b\'usqueda: \textbf{Sherlock}}
	\thispagestyle{empty}
	\begin{flushleft}
		\LARGE\bfseries Sistemas de Recuperaci\'on de Informaci\'on\\[2cm]
	\end{flushleft}
	\rule{\textwidth}{1pt}
	\vspace{2pt}
	\begin{flushright}
		\Huge
		\begin{tabular}{@{}l}
			Motor de \\
			b\'usqueda:\\
			\textbf{Sherlock}
			\includegraphics[height=1em]{sherlock.png}\\[6pt]
%			Modelos de\\ 
%			recuperaci\'on de\\ 
%			informaci\'on\\[6pt]
		\end{tabular}
	\end{flushright}
	\rule{\textwidth}{1pt}
	\vfill
	\begin{flushleft}
		\large\itshape
		\begin{tabular}{@{}l}
			{\large\upshape\bfseries Autores}\\[8pt]
			Laura Victoria Riera P\'erez\\[5pt]
			Leandro Rodr\'iguez Llosa\\[5pt]
			Marcos Manuel Tirador del Riego
		\end{tabular}
	\end{flushleft}
	
	\newpage
	\pagenumbering{gobble}
	\begin{abstract}
		Se abordan los aspectos principales de una posible implementaci\'on del modelo vectorial cl\'asico.
		
		\vspace{1em}
		\keywords{recuperaci\'on de informaci\'on (RI) \textbf{$\cdot$} modelo vectorial}
	\end{abstract} 
	\thispagestyle{empty}
	
	
	\newpage
	\pagenumbering{gobble}
	\tableofcontents
	\thispagestyle{empty}
	
	\newpage
	\pagenumbering{arabic}
	
	\section{Introducci\'on}
	
	En la actualidad, con el inmenso crecimiento del internet, se convierte en un reto cada vez mayor el manejo de la información, su recuperaci\'on y la extracci\'on de conocimiento de ella. Por esto, es de especial interes\'es la creaci\'on de algoritmos que ayuden a su manipulaci\'on. En este informe se expondr\'an algunas ideas importantes seguidas a la hora de implementar un modelo de recuperaci\'on de informaci\'on cl\'asico: el modelo vectorial.
	
	\section{Modelaci\'on del problema}
	
	\subsection{Documentos}
	Para el desarrollo de este Sistema de Recuperaci\'on de la Informaci\'on modelamos un documento como un objeto que contiene al menos dos propiedades, un \texttt{doc\_id} que identifica de manera \'unica a un documento dentro del conjunto de documentos del dataset en cuesti\'on; y un \texttt{text} que corresponde con el texto de este. Tambi\'en puede tener otras propiedades, por ejemplo: \texttt{title}, \texttt{author}; pero eso depende de la riqueza del dataset que provee el paquete de Python \texttt{ir\_datasets} (ver \cite{B3}).
	
	Como el texto de un documento es inc\'omodo de manipular por venir en forma de \texttt{string}, este se toqueniza y convierte en una lista de t\'erminos indexados normalizados. Esto se logra haciendo uso del paquete de Python \texttt{re} (referirse a \cite{B4}) que proporciona una colecci\'on de funciones que facilitan el trabajo con expresiones regulares. 
	
	\subsection{Normalizaci\'on de un t\'ermino}
	
	Es importante destacar algunas asunciones que se tuvieron en cuenta en el proceso de toquenizaci\'on. No se considera relevante la diferenciación entre una letra may\'uscula y una min\'uscula, ya que, en la mayoría de los casos, el significado sem\'antico que expresan es el mismo. Para reducir algunos errores ortográficos, y que esto no perjudique la recuperaci\'on de un documento importante, se considera que las tildes no diferencian una palabra de otra. Se conoce que esto \'ultimo no es cierto en el español, pero por el momento se est\'a trabajando con textos en ingl\'es.
	
	Resumiendo, se trata de llevar todos los t\'erminos a cadenas de caracteres que contienen letras min\'usculas o n\'umeros.
	
	\subsection{Corpus}
	
	Un corpus no es más que el conjunto de documentos de un dataset, toquenizados y normalizados.
	
	\subsection{Modelo base}
	
	Se define un modelo base como un concepto abstracto, que engloba los comportamientos comunes que debe tener cada modelo de recuperaci\'on de informaci\'on. 
	
	Cada instancia de un modelo tiene un corpus asociado a este, sobre el cual se hacen las b\'usquedas. Se deja en manos del programador qu\'e preprocesamientos hacer con el corpus, en dependencia del modelo que se est\'e implementando. 
	
	\section{Modelo vectorial}
	
	\subsection{Preprocesamiento}
	
	Seg\'un la f\'ormula de similitud del coseno (\cite[\emph{Ecuaci\'on}~(6.10)]{B1}), se puede notar que el aporte de un documento a la f\'ormula se mantiene invariante para todas las consultas, ya que este solo depende del vector que representa al documento, el cual es independiente de la consulta. Por tanto, como el corpus sobre el cu\'al se va a recuperar informaci\'on se mantendr\'a est\'atico, se calcula el peso de cada t\'ermino en cada documento para as\'i poder utilizarlo en cada consulta que se realice.
	
	Para calcular el peso de cada t\'ermino en cada documento seg\'un \cite[Ecuaci\'on (2.3)]{B1}, primero se necesita calcular las frecuencias normalizadas de los t\'erminos en cada documento (TF \cite[Ecuaci\'on (2.1)]{B2}), y la frecuencia de ocurrencia de cada t\'ermino dentro de todos los documentos del corpus (IDF \cite[Ecuaci\'on(2.2)]{B2}).
	
	\subsection{Recuperación de documentos}
	
	La primera fase es similar a la del preprocesamiento de los documentos del corpus. Lo primero que se hace es toquenizar y normalizar la consulta. Luego se hallan los TFs, y se calcula el peso de cada t\'ermino en la consulta. Para el c\'alculo de los pesos de cada t\'ermino  se utiliza la medida de suavizado \texttt{a = 0.4} para amortizar la contribuci\'on de la frecuencia del t\'ermino (ver \cite[ecuaci\'on (2.4)]{B2}). Por \'ultimo se halla la cercan\'ia entre el vector consulta y cada vector documento, utilizando la similitud del coseno entre los vectores seg\'un \cite[Ecuaci\'on 6.10]{B1}, y se hace un ranking teniendo este valor calculado.
	
	
	\section{Modelo Fuzzy}
	
		En el modelo booleano se representan las consultas y documentos como conjuntos de palabras. Al determinar que un documento es relevante a una consulta si y solo si tiene una coincidencia exacta de las palabras en la consulta solamente nos acercamos parcialmente al contenido sem\'antico real de los contenidos. Una alternativa ser\'ia definir un conjunto difuso por cada palabra de la consulta y determinar un grado de pertenencia de cada documento a este conjunto, para luego basado en esto, poder asignar un grado de relevancia de cada documento a la consulta dada. Esta es la idea b\'asica detr\'as de distintos modelos de recuperaci\'on de la informaci\'on basados en conjuntos difusos. 
		
		Se presentan a continuaci\'on algunos conceptos de la teor\'ia de conjuntos difusos necesario para entender el modelo implementado que se describe en esta secci\'on.
		\begin{defn} (\cite[Section~2.6.1]{B2}) Un conjunto difuso $A$ de un universo de discurso $U$ est\'a caracterizado por una funci\'on de membres\'ia $\mu_A : U \rightarrow [0,1]$ que asocia a cada elemento $u \in U$ un n\'umero $\mu_A(u)$ en el intervalo $[0,1].$
		\end{defn}
		Las tres operaciones m\'as usadas de conjuntos difusos se definen a continuaci\'on.
		\begin{defn} \label{def2} (\cite[Section~2.6.1]{B2})
			Sea $U$ el universo de discurso, $A$ y $B$ dos conjuntos difusos de $U$ y $\overline{A}$ el complemento de $A$ en $U$. Sea adem\'as un elemento $u \in U$. Entonces, 
			\begin{align*}
				\mu_{\overline{A}}(u) &= 1- \mu_A(u) \\
				\mu_{A \cup B}(u) &= \max(\mu_A(u), \mu_B(u))  \\
				\mu_{A\cap B}(u) &= \min(\mu_A(u), \mu_B(u)).
			\end{align*}
			
		\end{defn}
	
	
	\subsection{Descripci\'on del modelo usado}
	
	El modelo fuzzy que se seleccion\'o para su implementaci\'on fue propueto por Ogawa, Morita y Kobayashi en \cite{B5}. En este los autores se basan en el uso de la relaci\'on entre t\'erminos para expandir los t\'erminos de las consultas, de forma que se encuentren m\'as documentos que sean relevantes a las necesidades del usuario. Para expandir las consultas se opta por el uso de la matriz de conexi\'on de t\'erminos clave (keyword connection matrix). En esta matriz se encuentra la correlaci\'on normalizada entre cualesquiera dos t\'erminos $k_i$ y $k_j$ definida como
	\[
		c_{i,j} = \frac{n_{i,j}}{n_i + n_j - n_{i,j}},
	\]
	donde $n_i$ (respectivamente $n_j$) es el n\'umero de documentos que contienen a $k_i$ (respectivamente $k_j$) y $n_{i,j}$ es el n\'umero de documentos que los contienen a ambos. Esta correlaci\'on se basa en la idea de que si dos palabras est\'an relacionadas aparecer\'an, con frecuencia, juntas en un mismo documento.
	
	Definimos entonces la pertenencia de un documento $d_j$ al conjunto difuso relativo al t\'ermino $k_i$ como
	\[ \mu_{i,j} =  1 - \prod_{k_l \in d_j} (1 - c_{i,l}).\]
	Como podemos ver, luego de expandir el producto de la derecha, esta f\'ormula lo que computa es cierta suma algebraica de la correlaci\'on entre $k_l$ y todos los t\'erminos del documento $d_j$. Se observa que esta expresi\'on no es exactamente la suma directa de los $c_{i,l}$ para cada $k_l$, sino que es una suma suavizada. La expresi\'on resultante de expandir es similar a un principio de inclusi\'on exclusi\'on. Esto da la idea de que la relaci\'on entre $d_j$ y $k_i$ no es la suma de las correlaciones entre cada $k_l \in d_j$ y  $k_i$, sino que tiene en cuenta la relaci\'on conjunta con $k_i$ de los subconjuntos de t\'erminos de $d_j$, ya que puede que varios $k_l$ diferentes est\'en muy relacionados entre s\'i, y a $k_i$, y su aporte a la suma este siendo sobre valorado. 
	
	Aun con lo expuesto antes, cuando un t\'ermino  $k_l \in d_j$ est\'a muy relacionado con $k_i$ (es decir $c_{i,l} \approx 1$), entonces $\mu_{i,j} \approx 1$, lo cual dice que $d_j$ tiene un grado de pertenencia alto al conjunto difuso de $k_i$. Lo opuesto sucede cuando todos los t\'erminos $k_l \in d_j$ est\'an vagamente relacionados con $k_i$, en cuyo caso $c_{i,l} \approx 0$. Por tanto se mantiene el objetivo de la relaci\'on, aun cuando la suma usada no es la usual.
	
	Las consultas del usuario en este modelo vienen dadas de la misma forma que en el modelo booleano. Estamos hablando de una expresi\'on l\'ogica que se convierte luego a forma normal disyuntiva. Supongamos entonces que una consulta $q$ se descompone en las $n$ componentes conjuntivas $cc_1, cc_2 , \dots, cc_n$. Sea $\mu_{cc_t,j}$ el grado de pertenencia del documento $d_j$ al conjunto de documentos relevantes para la forma normal disyuntiva $c_t$. Este se calcula como sigue
	\[
	\mu_{cc_t,j} = \prod_{k_i \in cc_t} \mu'_{i,j}, 
	\]
	donde $\mu'_{i,j} = 1 - \mu_{i,j}$ si $k_i$ aparece negado en $cc_t$ y $\mu'_{i,j} =  \mu_{i,j}$ en otro caso. Entonces si denotamos $\mu_{q,j}$ como el grado de pertenencia del documento $d_j$ al conjunto de documentos relevantes a la consulta $q$, este se podr\'ia calcular como
	\[
	\mu_{q,j} = 1 - \prod_{t=1}^n (1 - \mu_{cc_t,j}).
	\]
	
	Seg\'un la Definici\'on~\ref{def2}, el valor $\mu_{cc_t,j}$ (el grado de pertenencia a una conjunto difuso conjuntivo) y $\mu_{q,j}$ (el grado de pertenencia a un conjunto difuso disyuntivo) deber\'ian ser calculados tomando el m\'inimo y el m\'aximo de las variables que intervienen respectivamente. Sin embargo se opta en este caso por usar producto y suma algebraica, respectivamente, para suavizar los resultados.
	
	\subsection{Implementaci\'on.}
		Para implementar el modelo descrito los autores del presente se basaron en el procesamiento de los documentos y consultas que se expusieron en el modelo booleano, ya que estas adoptan la misma forma. Se modifica entonces el m\'etodo que computa el resultado de una consulta, asignando en esta ocasi\'on una puntuaci\'on a cada documento  $d_j$ respecto a la consulta $q$, correspondiente a $\mu_{q,j}$. Los documentos ser\'an recuperados estableciendo un orden seg\'un este valor.
		
		Para calcular los valores en cada consulta de $\mu_{q,j}$ simplemente computamos las f\'ormulas descritas antes. La correlaci\'on entre t\'erminos, sin embargo, se calcula una sola vez para el total de pares de t\'erminos que aparecen en todos los documentos, y se guarda en el almacenamiento f\'isico de la computadora. Esto se hace para no tener que recalcular todos estos valores m\'as de una vez, ya que constituye un costo alto en tiempo.
	
	\section{Conclusiones}
	
	Este trabajo presenta una propuesta para la modelaci\'on del problema de recuperaci\'on de informaci\'on. Se detallaron aspectos generales del modelo vectorial cl\'asico, as\'i como decisiones de dise\~no espec\'ificas de la propia interpretaci\'on del problema.
	
	\begin{thebibliography}{20}
		\bibitem{B1} Maning C. D.: \emph{An Introduction To Information Retrieval} (2009).
		\bibitem{B2} Ricardo Baeza-Yates: \emph{Modern Information Retrieval} (1999).
		\bibitem{B3} Documentaci\'on oficial: \emph{https://ir-datasets.com/index.html}
		\bibitem{B4} Documentaci\'on oficial: \emph{https://docs.python.org/3/library/re.html}
		
		\bibitem{B5} Y. Ogawa, T. Morita y K. Kobayashi. A fuzzy document retrieval system using the keyword connection matrix and a learning method. Fuzzy Sets and Systems, 39:163-179, 1991.
	\end{thebibliography}
	
\end{document}
