% This is LLNCS.DOC the documentation file of
% the LaTeX2e class from Springer-Verlag
% for Lecture Notes in Computer Science, version 2.4
\documentclass{llncs}
\usepackage{Proyecto_Final_Sherlock}
\usepackage{color}


\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{amsfonts}
\usepackage{todonotes}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}

%\usepackage{amsthm}
\usepackage{amsmath}
\newtheorem{defn}[theorem]{Definici\'on}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage[spanish, english]{babel}

% Keywords command
\providecommand{\keywords}[1]
{
	\small	
	\textbf{\textit{Palabras clave --- }} #1
}

\providecommand{\enkeywords}[1]
{
	\small	
	\textbf{\textit{Keywords --- }} #1
}

\begin{document}
	\markboth{Sistemas de Recuperaci\'on de Informaci\'on}{	Motor de b\'usqueda: \textbf{Sherlock}}
	\thispagestyle{empty}
	\begin{flushleft}
		\LARGE\bfseries Sistemas de Recuperaci\'on de Informaci\'on\\[2cm]
	\end{flushleft}
	\rule{\textwidth}{1pt}
	\vspace{2pt}
	\begin{flushright}
		\Huge
		\begin{tabular}{@{}l}
			Motor de \\
			b\'usqueda:\\
			\textbf{Sherlock}
			\includegraphics[height=1em]{sherlock.png}\\[6pt]
%			Modelos de\\ 
%			recuperaci\'on de\\ 
%			informaci\'on\\[6pt]
		\end{tabular}
	\end{flushright}
	\rule{\textwidth}{1pt}
	\vfill
	\begin{flushleft}
		\large\itshape
		\begin{tabular}{@{}l}
			{\large\upshape\bfseries Autores}\\[8pt]
			Laura Victoria Riera P\'erez\\[5pt]
			Leandro Rodr\'iguez Llosa\\[5pt]
			Marcos Manuel Tirador del Riego
		\end{tabular}
	\end{flushleft}
	
	\newpage
	\pagenumbering{gobble}
	\selectlanguage{spanish} 
	\begin{abstract}
		A medida que aumenta el poder de cómputo y disminuye el costo de almacenamiento, la cantidad de datos que manejamos día a día crece exponencialmente. Pero sin una forma de recuperar la información y poder consultarla, la información que recopilamos se desperdicia. Con el inmenso crecimiento del internet, se convierte en un reto cada vez mayor el manejo de la información, su recuperaci\'on y la extracci\'on de conocimiento de ella. Por esto, es de especial interes\'es la creaci\'on de algoritmos que ayuden a su manipulaci\'on. En el presente trabajo se implement\'o un sistema de recuperaci\'on de informaci\'on mediante el motor de b\'usqueda "Sherlock". Se utilizaron tres colecciones de textos y se implementaron tres modelos de recuperaci\'on  de informaci\'on: el booleano y el vectorial cl\'asicos, y el fuzzy. Se realiz\'o una evaluaci\'on de los resultados obtenidos en cada uno de los modelos compar\'andolos con los de las colecciones de prueba. Adem\'as se a\~nadieron funcionalidades de retroalimentaci\'on, expansi\'on de consultas y agrupamiento. 
		
		\vspace{1em}
		\keywords{recuperaci\'on de informaci\'on (RI) \textbf{$\cdot$} motor de b\'usequeda \textbf{$\cdot$} modelo vectorial \textbf{$\cdot$} modelo booleano \textbf{$\cdot$} modelo fuzzy \textbf{$\cdot$} retroalimentaci\'on de Rocchio \textbf{$\cdot$} agrupamiento}
	\end{abstract} 
	
	\selectlanguage{english} 
	\begin{abstract}
		As computing power increases and the cost of storage decreases, the amount of day-to-day data we deal with is growing exponentially. However, without a way to retrieve the information and be able to query it, the data we collect most likely goes to waste. With the immense growth of the Internet, managing information, retrieving it, and extracting knowledge from it becomes an ever-increasing challenge. Therefore, it is of special interest the creation of algorithms that help its manipulation. In the present work, an information retrieval system was implemented through the "Sherlock" search engine. Three collections of texts were used and three information retrieval models were implemented: the classic Boolean and Vector models, and the Fuzzy model. An evaluation of the results obtained in each of the models was carried out, comparing them with those of the test collections. In addition, feedback, query expansion, and clustering features were added.
		
		\vspace{1em}
		\enkeywords{information retrieval (IR) \textbf{$\cdot$} search engine \textbf{$\cdot$} vector model \textbf{$\cdot$} boolean model \textbf{$\cdot$} fuzzy model \textbf{$\cdot$} Rocchio feedback \textbf{$\cdot$} clustering}
	\end{abstract} 
	\thispagestyle{empty}
	
	\selectlanguage{spanish} 
	\newpage
	\pagenumbering{gobble}
	\tableofcontents
	\thispagestyle{empty}
	
	\newpage
	\pagenumbering{arabic}
	
	\section{Introducci\'on}
	
	Un sistema de informaci\'on es ``un conjunto de componentes interrelacionados que permiten capturar, procesar, almacenar y distribuir la informaci\'on para apoyar la toma de decisiones y el control en una organizaci\'on".\footnote{C. Fleitas y M. S\'anchez, Conferencia 1 de Sistemas de la Recuperaci\'on de la Informaci\'on, Curso 2022, p. 12} \todo{chequear si les cuadra}
	Por otro lado, la recuperación de información se ocupa de la localizaci\'on  de materiales de naturaleza no estructurada en grandes repositorios de datos (generalmente documentos de texto).
	Se cree que la recuperación de información es la forma dominante de acceso a la información. 
	
	Actualmente, cientos de millones de personas utilizan la recuperaci\'on de informaci\'on todos los días cuando usan motores de búsqueda web. El sistema de recuperaci\'on de informaci\'on ayuda a los usuarios a encontrar la información que necesitan, notificando sobre la existencia y ubicación de documentos que pudieran constar de la información requerida.
	
	Un modelo de Recuperación de Información selecciona y clasifica el documento que el usuario ha solicitado en forma de consulta. Los documentos y las consultas se representan de manera similar, de modo que la selección y clasificación de documentos se puede formalizar mediante una función de coincidencia que devuelve un valor de similitud para cada documento de la colección. Muchos de los sistemas de Recuperación de Información representan el contenido de los documentos mediante un conjunto de términos, pertenecientes a un vocabulario.
	
	Los sistemas de recuperación de información son muy importantes para dar sentido a los datos. La información no es conocimiento sin sistemas de recuperación de información.
	
	En el presente trabajo se implement\'o un sistema de recuperaci\'on de informaci\'on mediante el motor de b\'usqueda web "Sherlock". Se utilizaron tres colecciones de textos y se implementaron tres modelos de recuperaci\'on  de informaci\'on: el booleano y el vectorial cl\'asicos, y el modelo difuso o fuzzy. Se realiz\'o una evaluaci\'on de los resultados obtenidos en cada uno de los modelos compar\'andolos con los de las colecciones de prueba. Adem\'as se a\~nadieron funcionalidades de retroalimentaci\'on, expansi\'on de consultas y agrupamiento. 
	
	\todo{P\'arrafo con estructura del doc ???}
	\section{Dise\~no del sistema}
	
	\subsection{Documentos}
	Para el desarrollo de este Sistema de Recuperaci\'on de la Informaci\'on modelamos un documento como un objeto que tiene dos propiedades, un \texttt{doc\_id} que identifica de manera \'unica a un documento dentro del conjunto de documentos del dataset en cuesti\'on; y un diccionario que posee el conjunto de términos indexados contra la frecuencia de los mismos en el documento.
	
	Como el texto de un documento es inc\'omodo de manipular por venir en forma de \texttt{string}, este se toqueniza y convierte en una lista de t\'erminos indexados normalizados. Esto se logra haciendo uso del paquete de Python \texttt{re} (referirse a \cite{B4}) que proporciona una colecci\'on de funciones que facilitan el trabajo con expresiones regulares. 
	
	\subsection{Normalizaci\'on de un t\'ermino}
	
	Es importante destacar algunas asunciones que se tuvieron en cuenta en el proceso de toquenizaci\'on. No se considera relevante la diferenciación entre una letra may\'uscula y una min\'uscula, ya que, en la mayoría de los casos, el significado sem\'antico que expresan es el mismo. Para reducir algunos errores ortográficos, y que esto no perjudique la recuperaci\'on de un documento importante, se considera que las tildes no diferencian una palabra de otra. Se conoce que esto \'ultimo no es cierto en el español, pero por el momento se est\'a trabajando con textos en ingl\'es.
	
	Resumiendo, se trata de llevar todos los t\'erminos a cadenas de caracteres que contienen letras min\'usculas o n\'umeros.
	
	\subsection{Corpus}
	
	Un corpus no es más que el conjunto de documentos de un dataset, toquenizados y normalizados.
	
	\subsection{Modelo base}
	
	Se define un modelo base como un concepto abstracto, que engloba los comportamientos comunes que debe tener cada modelo de recuperaci\'on de informaci\'on. 
	
	Cada instancia de un modelo tiene un corpus asociado a este, sobre el cual se hacen las b\'usquedas. Se deja en manos del programador qu\'e preprocesamientos hacer con el corpus, en dependencia del modelo que se est\'e implementando, cómo guardar y cargar estos preprocesamientos. Esto último se logra con el paquete de Python \texttt{dictdatabase} (\cite{B7}), que permite un manejo fácil con \textit{JSON}.
	
	\section{Modelo Booleano}
	
	El modelo booleano cl\'asico expuesto en \cite[Secci\'on 2.2.5]{B2} es un modelo simple de Recuperaci\'on de Informaci\'on basado en teor\'ia de conjuntos y \'algebra booleana. Las consultas son expresiones booleanas que utilizan los operadores l\'ogicos AND, OR y NOT, y s\'olo se recuperan los documentos que tengan coincidencias exactas a las mismas. Es un modelo eficiente y formal, de f\'acil comprensi\'on e implementaci\'on, recomendado para el trabajo con expertos sobre un tema espec\'ifico.
	
	\subsection{Descripci\'on del modelo}
	
	Para el modelo booleano, los pesos de los t\'erminos indexados son binarios, es decir, $ w_{ij} \in \{0,1\} $.
	
	\begin{defn}
		\cite{B2}
		Sea $ q $ una consulta (expresi\'on booleana convencional), $ q_{fnd} $ la forma normal disyuntiva de la consulta $ q $. Además, sea $ q_{cc} $ cualquiera de los componentes conjuntivos de $ q_{fnd} $. La similitud de un documento $ d_{j} $ con la consulta $ q $ se define como:
		
		\begin{equation}
			sim(d_{j}, q)=
			\begin{cases}
				1, & \text{si}\ \exists  q_{cc} | (q_{cc} \in q_{fnd}) \wedge (\forall k_{i}, g_{i}(d_{j}) = g_{i}(q_{cc})) \\
				0, & \text{en otro caso}
			\end{cases}
		\end{equation}
		
		Si $ sim(dj, q) = 1 $, entonces el modelo booleano predice que el documento $ dj $ es relevante a la consulta $ q $ (podría no ser). De lo contrario, la predicción es que el documento no es relevante y este no es devuelto.
	\end{defn}
	
	\subsection{Implementaci\'on}

	La consulta debe ser una expresi\'on booleana convencional, donde se utilizan los s\'imbolos $ \& $, $ | $ y $ \sim $  para representar las operaciones AND, OR y NOT respectivamente. En caso de que no aparezca ning\'un operador se asume que se desea que todas las palabras aparezcan en el documento y, por tanto, se toman con AND entre ellas. 
	
	Para procesar la consulta se eliminan de la misma todos los caracteres innecesarios, quedando solo letras, los s\'imbolos de los operadores y par\'entesis (para poder agrupar t\'erminos). Para convertir la consulta de string a expresi\'on y hallar entonces su forma normal disyuntiva se utilizan los m\'etodos sympify y to\_dfn de la biblioteca sympy. Finalmente, se tendr\'a una lista en donde en cada posici\'on se tiene otra lista con todos los t\'erminos de una componente conjuntiva.
	
	Para hallar las coincidencias a documentos simplemente se recorre cada componente conjuntiva y se a\~naden a la respuesta los documentos que contengan a todos los t\'erminos de la misma con un score igual a 1.  
	
	\section{Modelo Vectorial}

    En el modelo vectorial cada documento se representa como un vector de términos indexados, donde lo único que interesa saber es qué términos aparecen en el documento y qué pesos tiene cada uno de esos términos en el documento. Luego dada una consulta, que también se representa como un documento, hallar el conjunto de documentos relevantes se reduce a encontrar el conjunto de vectores documento más similares a la consulta.

    Para entender mejor estas ideas, se necesitan conocer que es \emph{TF * IDF}.
        
    \subsubsection{TF} La frecuencia de un término en un documento se halla calculando la cantidad de ocurrencias del término en el documento dividido por la máxima cantidad de ocurrencias que obtiene algún término en ese documento. Esta idea de dividir por la máxima cantidad de ocurrencias pretende normalizar los valores de frecuencia para evitar que documentos muy extensos obtengan mayores valores de frecuencia que otros documentos más pequeños. 

    \begin{equation}
        TF_{t_i,d_j} = \frac{freq(t_i, d_j)}{\texttt{max} freq (t_k, d_j)} 
    \end{equation}

    \begin{center}
        $t_i$ - i-ésimo término\\
        $d_j$ - j-ésimo documento\\
        $freq(t_i, d_j)$ - cantidad de ocurrencias de $t_i$ en $d_j$
    \end{center}

    Con TF solamente no basta para saber la relevancia que tiene un término en un documento porque las palabras muy comunes en un texto, como por ejemplo las preposiciones, aparecerían con un TF alto en todos los documentos, por tanto se les daría mayor relevacia a estas palabras, que no tienen mucha importancia, con a otras que quizás son palabras claves para la consulta. Idealmente se quisiera penalizar este tipo de palabras tan frecuentes en todos los textos, ese es el objetivo de \emph{IDF}.

    \subsubsection{IDF} La frecuencia inversa de un documento calcula la proporción de documentos en los que está un término contra la cantidad total de documentos. Luego se halla el inverso para obtener mayores valores mientras en menos documentos esté el término, bajo el supuesto de que mientras en menos documentos esté, mayor rareza tiene y por tanto más importante debe ser ese término si aparece en la consulta.

    \begin{equation}
        IDF_{t_i} = \log \frac{N}{n_i}
    \end{equation}

    \begin{center}
        $t_i$ - i-ésimo término\\
        $N$ - cantidad total de documentos\\
        $n_i$- cantidad de documentos en los que aparece $t_i$ 
    \end{center}

    Teniendo estas dos medidas es posible formular el peso de un término en un documento, dado que se conoce la frecuencia de cada término en un documento y la frecuencia inversa de un término dentro del conjunto de documentos. 

    \begin{equation}
        W_{t_i, d_j} = TF_{t_i, d_j} * IDF_{t_i}
    \end{equation}

    En el caso del vector consulta, al calculo de los pesos del término se le adiciona una constante de suavizado para amortiguar la variación en los pesos de términos que ocurren poco y evitar grandes saltos entre la frecuencia de un término que aparece una vez y otro que aparece dos veces.

    \begin{equation}
        W_{t_i, q} = (a + (1-a) * TF_{t_i, q}) * IDF_{t_i}
    \end{equation}
        
    Por tanto si se quisiera hacer una consuta, idealmente se quiere recuperar documentos que tenga términos de la consulta que sean poco comunes dentro del conjunto de documentos, y a la vez tengan una frecuencia alta dentro del documentos recuperados. Ahora la pregunta interesante sería cómo encontrar esos documentos que cumplen esto y cómo ordenarlos por su nivel de relevancia.

    \subsubsection{Similitud del coseno} Para calcular la correlación entre el vector consulta y un vector documento se calcula el coseno del ángulo entre estos dos vectores, obteniendo valores cercanos a 1 mientras más cercanos sean, y cercanos a -1 mientras más distintos sean. Con esto se logra obtener una función de ranking, que le da mayor peso a documentos que poseen términos poco comunes dentro del conjunto de documentos y una alta frecuencia dentro del documento. 

    \begin{equation}
        \texttt{sim} (\Vec{d_j}, \Vec{q}) = \frac{\Vec{d_j} * \Vec{q}}{ \| \Vec{d_j} \| * \| \Vec{q} \| }
    \end{equation}

    \begin{center}
        $d_j$ - vector del j-ésimo documento\\
        $q$ - vector consulta
    \end{center}

    Para encontrar una explicación más detallada de como funcionan estás ideas, se sugiere leer \cite[epígrafe 6]{B1}.

    \subsection{Implementación}

    \subsubsection{Preprocesamiento}
	
	Seg\'un la f\'ormula de similitud del coseno (\cite[\emph{Ecuaci\'on}~(6.10)]{B1}), se puede notar que el aporte de un documento a la f\'ormula se mantiene invariante para todas las consultas, ya que este solo depende del vector que representa al documento, el cual es independiente de la consulta. Por tanto, como el corpus sobre el cu\'al se va a recuperar informaci\'on se mantendr\'a est\'atico, se calcula el peso de cada t\'ermino en cada documento para as\'i poder utilizarlo en cada consulta que se realice.
	
	Para calcular el peso de cada t\'ermino en cada documento seg\'un \cite[Ecuaci\'on (2.3)]{B1}, primero se necesita calcular las frecuencias normalizadas de los t\'erminos en cada documento (TF \cite[Ecuaci\'on (2.1)]{B2}), y la frecuencia de ocurrencia de cada t\'ermino dentro de todos los documentos del corpus (IDF \cite[Ecuaci\'on(2.2)]{B2}).
	
	\subsubsection{Recuperación de documentos}
	
	La primera fase es similar a la del preprocesamiento de los documentos del corpus. Lo primero que se hace es toquenizar y normalizar la consulta. Luego se hallan los TFs, y se calcula el peso de cada t\'ermino en la consulta. Para el c\'alculo de los pesos de cada t\'ermino  se utiliza la medida de suavizado \texttt{a = 0.4} para amortizar la contribuci\'on de la frecuencia del t\'ermino (ver \cite[ecuaci\'on (2.4)]{B2}). Por \'ultimo se halla la cercan\'ia entre el vector consulta y cada vector documento, utilizando la similitud del coseno entre los vectores seg\'un \cite[Ecuaci\'on 6.10]{B1}, y se hace un ranking teniendo este valor calculado.

    \subsection{Retroalimentaci\'on}

    Con el objetivo de mejorar los resultados del sistema es bueno involucrar al usuario en el proceso de recuperación de información. Para ello el usuario debe emitir un clasificación de algunos documentos recuperados, en relevantes o no, dada la consulta realizada, y de esta forma poder descartar o incluir a otros documentos. El algoritmo utilizado para esto es el de Rocchio, a continuación se explicará brevemente.

    \subsubsection{Algoritmo de Rocchio} 
        
    Este consiste en acercar el vector que representa a la consulta con aquellos que representan a los documentos clasificados como relevantes por el usuario y alejarlo de los no relevantes. Así mediante este proceso se va modificando el vector de la consulta inicial, hasta encontrar un vector consulta que satisfaga las necesidades del usuario. El vector $q_m$ de la consulta se transforma en el vector:

    \begin{equation}
        q_m = \alpha * q_0 + \frac{\beta}{\| D_r \|} *  \sum_{d_i \in D_r} d_i - \frac{\gamma}{\| D_{nr} \|} *  \sum_{d_j \in D_{nr}} d_j
    \end{equation}

    \begin{center}
        $q_m$ - vector consulta modificado\\
        $q_0$ - vector consulta inicial\\
        $D_r$ - conjunto de documentos relevantes\\
        $D_{nr}$ - conjunto de documentos no relevantes\\
        $\alpha$ - peso asignado al vector consulta inicial\\
        $\beta$ - peso asignado al conjunto de documentos clasificados como relevantes\\
        $\gamma$ - peso asignado al conjunto de documentos clasificados como no relevantes\\
    \end{center}

    Para obtener más información sobre este algoritmo se propone consultar a \cite[epígrafe 9.1.1]{B1}.
	
	\section{Modelo Fuzzy}
	
		En el modelo booleano se representan las consultas y documentos como conjuntos de palabras. Al determinar que un documento es relevante a una consulta si y solo si tiene una coincidencia exacta de las palabras en la consulta, solamente nos acercamos parcialmente al contenido sem\'antico real de la informaci\'on. Una alternativa ser\'ia definir un conjunto difuso por cada palabra de la consulta y determinar un grado de pertenencia de cada documento a este conjunto, para luego basado en esto, poder asignar un grado de relevancia de cada documento a la consulta dada. Esta es la idea b\'asica detr\'as de distintos modelos de recuperaci\'on de la informaci\'on basados en conjuntos difusos. 
		
		Se presentan a continuaci\'on algunos conceptos de la teor\'ia de conjuntos difusos necesarios para entender el modelo implementado que se describe en esta secci\'on.
		\begin{defn} (\cite[Section~2.6.1]{B2}) Un conjunto difuso $A$ de un universo de discurso $U$ est\'a caracterizado por una funci\'on de membres\'ia $\mu_A : U \rightarrow [0,1]$ que asocia a cada elemento $u \in U$ un n\'umero $\mu_A(u)$ en el intervalo $[0,1].$
		\end{defn}
		Las tres operaciones m\'as usadas de conjuntos difusos se definen a continuaci\'on.
		\begin{defn} \label{def2} (\cite[Section~2.6.1]{B2})
			Sea $U$ el universo de discurso, $A$ y $B$ dos conjuntos difusos de $U$ y $\overline{A}$ el complemento de $A$ en $U$. Sea adem\'as un elemento $u \in U$. Entonces, 
			\begin{align*}
				\mu_{\overline{A}}(u) &= 1- \mu_A(u) \\
				\mu_{A \cup B}(u) &= \max(\mu_A(u), \mu_B(u))  \\
				\mu_{A\cap B}(u) &= \min(\mu_A(u), \mu_B(u)).
			\end{align*}
			
		\end{defn}
	
	
	\subsection{Descripci\'on del modelo usado}
	
	El modelo fuzzy que se seleccion\'o para su implementaci\'on fue propueto por Ogawa, Morita y Kobayashi en \cite{B5}. En este los autores se basan en el uso de la relaci\'on entre t\'erminos para expandir los t\'erminos de las consultas, de forma que se encuentren m\'as documentos que sean relevantes a las necesidades del usuario. Para expandir las consultas se opta por el uso de la matriz de conexi\'on de t\'erminos clave (keyword connection matrix). En esta matriz se encuentra la correlaci\'on normalizada entre cualesquiera dos t\'erminos $k_i$ y $k_j$ definida como
	\[
		c_{i,j} = \frac{n_{i,j}}{n_i + n_j - n_{i,j}},
	\]
	donde $n_i$ (respectivamente $n_j$) es el n\'umero de documentos que contienen a $k_i$ (respectivamente $k_j$) y $n_{i,j}$ es el n\'umero de documentos que los contienen a ambos. Esta correlaci\'on se basa en la idea de que si dos palabras est\'an relacionadas aparecer\'an, con frecuencia, juntas en un mismo documento.
	
	Se define entonces la pertenencia de un documento $d_j$ al conjunto difuso relativo al t\'ermino $k_i$ como
	\[ \mu_{i,j} =  1 - \prod_{k_l \in d_j} (1 - c_{i,l}).\]
	Como se puede ver, luego de expandir el producto de la derecha, esta f\'ormula lo que computa es cierta suma algebraica de la correlaci\'on entre $k_l$ y todos los t\'erminos del documento $d_j$. Se observa que esta expresi\'on no es exactamente la suma directa de los $c_{i,l}$ para cada $k_l$, sino que es una suma suavizada. La expresi\'on resultante de expandir es similar a un principio de inclusi\'on exclusi\'on. Esto da la idea de que la relaci\'on entre $d_j$ y $k_i$ no es la suma de las correlaciones entre cada $k_l \in d_j$ y  $k_i$, sino que tiene en cuenta la relaci\'on conjunta con $k_i$ de los subconjuntos de t\'erminos de $d_j$, ya que puede que varios $k_l$ diferentes est\'en muy relacionados entre s\'i, y relacionados con  $k_i$, y su aporte a la suma este siendo sobrevalorado. 
	
	Aun no us\'andose la suma directa, se sigue cumpliendo que cuando un t\'ermino  $k_l \in d_j$ est\'a muy relacionado con $k_i$ (es decir $c_{i,l} \approx 1$), entonces $\mu_{i,j} \approx 1$, lo cual dice que $d_j$ tiene un grado de pertenencia alto al conjunto difuso de $k_i$. Lo opuesto sucede cuando todos los t\'erminos $k_l \in d_j$ est\'an vagamente relacionados con $k_i$, en cuyo caso $c_{i,l} \approx 0$. Por tanto se mantiene el objetivo de la relaci\'on, aun cuando la suma usada no es la usual.
	
	Las consultas del usuario en este modelo vienen dadas de la misma forma que en el modelo booleano. Estamos hablando de una expresi\'on l\'ogica que se convierte luego a forma normal disyuntiva. Sup\'ongase entonces que una consulta $q$ se descompone en las $n$ componentes conjuntivas $cc_1, cc_2 , \dots, cc_n$. Sea $\mu_{cc_t,j}$ el grado de pertenencia del documento $d_j$ al conjunto de documentos relevantes para la forma normal disyuntiva $cc_t$. Este se calcula como sigue
	\[
	\mu_{cc_t,j} = \prod_{k_i \in cc_t} \mu'_{i,j}, 
	\]
	donde $\mu'_{i,j} = 1 - \mu_{i,j}$ si $k_i$ aparece negado en $cc_t$ y $\mu'_{i,j} =  \mu_{i,j}$ en otro caso. Entonces si se denota $\mu_{q,j}$ como el grado de pertenencia del documento $d_j$ al conjunto de documentos relevantes a la consulta $q$, este se podr\'ia calcular como
	\[
	\mu_{q,j} = 1 - \prod_{t=1}^n (1 - \mu_{cc_t,j}).
	\]
	
	Seg\'un la Definici\'on~\ref{def2}, el valor $\mu_{cc_t,j}$ (el grado de pertenencia a una conjunto difuso conjuntivo) y $\mu_{q,j}$ (el grado de pertenencia a un conjunto difuso disyuntivo) deber\'ian ser calculados tomando el m\'inimo y el m\'aximo de las variables que intervienen respectivamente. Sin embargo, se opta en este caso por usar producto y suma algebraica, respectivamente, para suavizar los resultados.
	
	\subsection{Implementaci\'on.}
		Para implementar el modelo descrito los autores del presente se basaron en el procesamiento de los documentos y consultas que se expusieron en el modelo booleano, ya que estas adoptan la misma forma. Se modifica entonces el m\'etodo que computa el resultado de una consulta, asignando en esta ocasi\'on una puntuaci\'on a cada documento  $d_j$ respecto a la consulta $q$, correspondiente a $\mu_{q,j}$. Los documentos ser\'an recuperados estableciendo un orden seg\'un este valor.
		
		Para calcular los valores en cada consulta de $\mu_{q,j}$ simplemente se computan las f\'ormulas descritas antes. La correlaci\'on entre t\'erminos, sin embargo, se calcula una sola vez para el total de pares de t\'erminos que aparecen en todos los documentos, y se guarda en el almacenamiento f\'isico de la computadora. Esto se hace para no tener que recalcular todos estos valores m\'as de una vez, ya que constituye un costo alto en tiempo.
	
	\section{Evaluaci\'on de los modelos}
	
	Se han presentado en este trabajo tres alternativas a modelos usados para implementar un sistema de recuperaci\'on de la informaci\'on. Es importante entonces contar con una forma de calcular la efectividad de cada uno de estos, de forma que podamos compararlos en sus distintas aplicaciones.
	
	Para medir la efectividad de cualquier modelo se necesita de una colecci\'on de  prueba, que no es m\'as que un conjunto de documentos sobre el que se puede ejecutar el motor de b\'usqueda con el modelo en cuesti\'on. Esta colecci\'on debe contar con un conjunto de necesidades de informaci\'on, expresadas en forma de consulta, as\'i como con una evaluaci\'on de cada documento de la colecci\'on seg\'un su relevancia respecto a cada consulta.\footnote{Idea tomada de \cite[Secci\'on 8.1]{B1}} En el sistema implementado se usaron los siguientes tres colecciones de prueba
	
	
	
	\subsection{Modelo Booleano}
	
	\begin{center}
		\includegraphics[width=10cm]{cranfield_boolean}
		
		\includegraphics[width=10cm]{vaswani_boolean}
		
		\includegraphics[width=10cm]{cord19_boolean}
	\end{center}
	
	\subsection{Modelo Vectorial}

    El modelo vectorial no se comporta bien en corpus de dominio específico. Tanto \emph{Cranfield}, como \emph{Vaswani} y \emph{Cord19} son corpus de dominio específico, por lo que las evaluaciones no son muy acertadas.

    \begin{center}
        \includegraphics[width=10cm]{PR_plot(k=300).png}    
    \end{center}

    En esta gráfica se muestra hasta los primeros 300 documentos, como se comporta el modelo con los 3 corpus. 
        
    \emph{Cord19} comienza con una precisión bastante alta, puesto que es un corpus grande, y el modelo puede distinguir mejor entre palabras que sean comunes en muchos documentos y aquellas que no lo son. Esto como bien sabemos es una ventaja y desventaja a la vez, puesto que como tiene muchos documentos, deben haber una mayor cantidad de documentos relevantes y en las primeras iteraciones se logra recuperar menos, por lo que el recobrado da muy bajo. Esto a su vez afecta bastante a la medida F1. Pero a medida que va disminuyendo la precisión y aumentando el recobrado se logra un mejor equilibrio de ambos para $k = 71$, con $F1=0.2196$, $P=0.2192$ y $R=0.2201$.

    Por otra parte, \emph{Vaswani} y \emph{Cranfield} se comportan bastante parecidos. La diferencia radica en que la precisión de \emph{Vaswani} baja con mayor rapidez que la de \emph{Cranfield} ya que es un corpus más grande y para subir el recobrado necesita de mayor cantidad de documentos a recolectar, y con este aumento de la cantidad de documentos disminuye la precisión porque este modelo como no tiene en cuenta la correlación entre los términos y en corpus de dominio específico esto influye bastante. 
        
    Los mejores resultados para \emph{Cranfield} se obtienen en $k = 8$ con $F1 = 0.2472$, $P = 0.2111$ y $R = 0.2982$; y para \emph{Vaswani} en $k = 29$ con $F1 = 0.1902$, $P = 0.1561$ y $R = 0.2435$ .

    \begin{center}
        \includegraphics[width=10cm]{PR_plot(all).png}    
    \end{center}

	\subsection{Modelo Fuzzy}
	
	Al ser un modelo que extiende al modelo booleano, aunque resuelve muchas de las dificultades del anterior, este aun acarrea otras de sus deficiencias. Una de estas principales dificultades es que el lenguaje de consultas es complejo para inexpertos. Las conjuntos de consultas de prueba tanto de \emph{Cranfield} como de \emph{Vaswani} no est\'an pensadas para aplicarlas sobre un modelo booleano. Esto se puede apreciar del simple hecho de que las mismas constituyen oraciones y no expresiones l\'ogicas.
	
	Sin embargo, si se puede apreciar una ligera mejor\'ia respecto al modelo booleano (en el caso de \emph{Crenfield} se obtiene un valor de la F-medida casi cuatro veces mayor). Este resultado es de esperar ya que este modelo mejora algunas de las dificultades del anterior, como por ejemplo que en este caso la coincidencia de los documentos no tiene que ser exacta, se crea un ranking, se adiciona cierta sem\'antica a los t\'erminos analizando cierta correlaci\'on entre los pares de ellos y adem\'as, de esto \'ultimo se puede inferir que no todos los t\'erminos seguir\'an siendo igual de importantes.
	 
	En el caso del conjunto de datos \emph{cord19} este no pudo usarse en el modelo fuzzy ya que la gran cantidad de documentos y t\'erminos que posee este es muy grande, lo que hace que calcular la correlaci\'on entre cada par de t\'erminos incurra en un uso de recursos muy elevado. Si dicha correlaci\'on se precalcula, har\'ian falta m\'as de $20 GB$ de RAM seg\'un los c\'alculos hechos (que no se presentan pues son aproximaciones poco formales). Si en cambio se hace en el momento de cada consulta solo para los t\'erminos de la consulta, ejecutar cada consulta tomar\'ia m\'as de $5$ minutos. Adem\'as, el modelo fuzzy no has sido extensamente probado en experimentos con colecciones grandes de documentos\footnote{Esta idea se tom\'o de \cite{B2}, p\'agina $38$.}.
	
	\begin{center}
		\includegraphics[width=10cm]{cranfield_fuzzy}
		
		\includegraphics[width=10cm]{vaswani_fuzzy}
	\end{center}	
	
    \section{Agrupamiento}
	
	Los algoritmos de agrupamiento, como su nombre lo indica, agrupan un conjunto de documentos en subconjuntos o clústeres. Los grupos formados deben tener un alto grado de asociación entre los documentos de un mismo grupo, es decir, deben ser lo m\'as similares posibles, y un bajo grado entre miembros de diferentes grupos. Es la forma más común de \textit{aprendizaje no supervisado}, es decir no hay ningún experto humano que haya asignado documentos a clases. Es la distribución y composición de los datos lo que determinará la pertenencia al clúster.\footnote{Las ideas de esta introducci\'on a la secci\'on fueron tomada de \cite{B6} de los propios autores de este informe.}
	
	Se presenta a continuaci\'on la hip\'otesis en que se basan los algoritmos de agrupamiento, que fue tomada de \cite[Secci\'on 16.1]{B1}.
	
	\vspace{1em}
	\textbf{Hip\'otesis de agrupamiento:} \textit{Los documentos en el mismo grupo se comportan de manera similar con respecto a la relevancia para las necesidades de información.}
	
	\vspace{0.3em}
	La hipótesis establece que si hay un documento de un grupo que es relevante a una solicitud de búsqueda, entonces es probable que otros documentos del mismo clúster también sean relevantes. 
	\vspace{1em}
	
	\subsection{K-means}
		En este trabajo se opt\'o por usar uno de los algoritmos de agrupamiento m\'as importantes, conocido como K-means. Este algoritmo se ejecuta sobre un conjunto espacio de documentos representados como vectores dimensionales. El mismo fija inicialmente de forma aleatoria los centroides de los cl\'usteres y en cada iteraci\'on los va moviendo de forma que se disminuya en cada paso la suma de los cuadrados de las distancias de cada documento a su cl\'uster m\'as cercano (RSS).
		
		Se define el centroide de un cl\'uster formalmente como 
		\[
		\overrightarrow{\mu}(w_k) = \frac{1}{|w|}\sum_{x \in w_k} \overrightarrow{x},
		\]
		donde $w_k$ es el cl\'uster y $\overrightarrow{x}$ es un vector que representa a un documento que pertenece a $w$. Se define RSS como 
		\[
			RSS = \sum_{i = 1}^K \sum_{\overrightarrow{x} \in w_k} |\overrightarrow{x} - \overrightarrow{\mu}(w_k)|^2,
		\]
		donde $K$ es el n\'umero de cl\'usteres.
	
	\subsection{Objetivo perseguido}
	
	Se decidi\'o usar este algoritmo de agrupamiento para mejorar la recuperaci\'on de documentos en el modelo vectorial. La idea perseguida es que seg\'un la hip\'otesis de agrupamiento se espera que documentos en \'el cl\'uster m\'as cercano al vector de la consulta debe contener con mayor probabilidad documentos similares a la consulta. Entonces reordenamos y reasignamos puntuaciones de relevancia a los documentos, de forma que sean m\'as relevantes aquellos que pertenezcan a cl\'usteres m\'as cercanos a la consulta, manteniendo el orden relativo que ten\'ian antes aquellos documentos en el mismo cl\'uster. Adem\'as, podemos presentar al usuario los documentos agrupados en los cl\'usteres, siendo m\'as f\'acil escanear algunos grupos
	coherentes que muchos documentos individuales. Esto es particularmente \'util
	si un t\'ermino de b\'usqueda tiene diferentes significados.
	 
	\subsection{Implementaci\'on}
	Para poner en pr\'actica la idea perseguida, se implement\'o un nuevo modelo de recuperaci\'on de la informaci\'on que se sustenta en el modelo vectorial antes explicado. Este modifica el m\'etodo de recuperaci\'on de documentos a una consulta para a\~nadir el criterio por cl\'usteres descrito. Los autores se auxiliaron de la biblioteca de python \emph{sk-learn}.

	 Una de las decisiones de implementaci\'on que se tuvo que tomar fue la elecci\'on de un n\'umero $k$ de cl\'usteres adecuados. Se implement\'o primero el conocido m\'etodo del codo que toma la decisi\'on en funci\'on de la gr\'afica de los valores de RSS para cada elecci\'on de $k$ considerada. Sin embargo este m\'etodo no arroj\'o ninguna luz en el conjunto de datos de \emph{Cranfield} el cual se us\'o para implementar este modelo. Entonces se opt\'o por una decisi\'on en funci\'on de minimizar los RSS, pero penalizando el n\'umero de cl\'usteres usados. La expresi\'on que se utiliz\'o fue la siguiente
	 \[
		k = \argmin_{k \in \mathbb{N}} (RSS_k + \lambda * k),
	 \]
	 donde $\lambda$ se tomo como $0.08$ multiplicado por la cantidad de documentos del corpus. Seg\'un esta expresi\'on un buen n\'umero de cl\'usteres (inferior a $10$ porque para valores mucho m\'as grandes el algoritmo tomaba mucho tiempo en ejecutarse) es $k=5$, y este es el utilizado.
	 
	 Para determinar la nueva puntuaci\'on de cada documento respecto a una consulta se toma 
	 \[score_d =  \frac{min\_distance - 1 + oldscore}{|\overrightarrow{q} - \overrightarrow{\mu_d}|},\]
	 donde $\overrightarrow{q}$ es el vector consulta, $\overrightarrow{\mu}$ es el centroide al que pertenece el documento $d$ y
	 \[ min\_distance = \min_{d}(|\overrightarrow{q} - \overrightarrow{\mu_d}|).\]
	 De esta forma siempre obtienen mejor puntuaci\'on los documentos de los cl\'usteres m\'as cercanos a la consulta y se mantiene el orden relativo que ten\'ian los documentos de un mismo cl\'uster seg\'un el modelo vectorial.
	 
	 \subsection{Resultados}
	 
	 En su aplicaci\'on en los conjuntos de datos de \emph{Cranfield} y \emph{Vaswani}, y su evaluaci\'on, este modelo tuvo resultados muy similares a los del modelo vectorial. Por tanto, una conclusi\'on a la que arribamos es que la aplicaci\'on de K-means en la forma descrita no constituye una mejora del modelo vectorial. Estos resultados son coherentes con la idea intuitiva de que si los documentos recuperados con el nuevo modelo son todos de un mismo cl\'uster, entonces son cercanos entre s\'i, y por tanto cercanos al vector consulta, por lo que deber\'ian ser recuperados tambi\'en en el modelo vectorial cl\'asico.
	 
	 Sin embargo, se tienen otras ideas de como aplicar este algoritmo al mismo modelo, sobre la base del mismo c\'odigo, que se seguir\'an probando en busca de mejorar los resultados.
	 
	\section{Conclusiones y trabajo futuro}
	
	Este trabajo presenta una propuesta para la modelaci\'on del problema de recuperaci\'on de informaci\'on. Se detallaron aspectos generales del modelo vectorial cl\'asico, as\'i como decisiones de dise\~no espec\'ificas de la propia interpretaci\'on del problema. 
	
	\begin{thebibliography}{20}
		\bibitem{B1} Maning C. D.: \emph{An Introduction To Information Retrieval} (2009).
		\bibitem{B2} Ricardo Baeza-Yates: \emph{Modern Information Retrieval} (1999).
		\bibitem{B3} Documentaci\'on oficial: \emph{https://ir-datasets.com/index.html}
		\bibitem{B4} Documentaci\'on oficial: \emph{https://docs.python.org/3/library/re.html}
		
		\bibitem{B5} Y. Ogawa, T. Morita y K. Kobayashi. A fuzzy document retrieval system using the keyword connection matrix and a learning method. Fuzzy Sets and Systems, 39:163-179, 1991.		
		\bibitem{B6} Laura Riera P\'erez y Marcos Tirador del Riego. Aplicaciones del
		agrupamiento y de la clasificación en la recuperaci\'on	de informaci\'on en la Web.
	\end{thebibliography}
	
\end{document}
